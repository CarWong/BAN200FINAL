{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1054e49a",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab15c1fc-e0e6-4695-be26-340d6e41dad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import ssl\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "import emoji\n",
    "from textblob import TextBlob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf6a3ca",
   "metadata": {},
   "source": [
    "### Downloading required NLTK data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5e219bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading required NLTK data...\n",
      "NLTK data downloaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Handle SSL certificate issues for NLTK downloads\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# Download required NLTK data\n",
    "print(\"Downloading required NLTK data...\")\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    print(\"NLTK data downloaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading NLTK data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee34e2f",
   "metadata": {},
   "source": [
    "### Combining all reviews into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bf41ff0-a828-488c-87ce-a9811fd080ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewsbmo = pd.read_json('reviews/com.bmo.mobile.json')\n",
    "reviewsbmo.insert(0, 'bank', 'BMO')\n",
    "reviewscibc = pd.read_json('reviews/com.cibc.android.mobi.json')\n",
    "reviewscibc.insert(0,'bank', 'CIBC')\n",
    "reviewsrbc = pd.read_json('reviews/com.rbc.mobile.android.json')\n",
    "reviewsrbc.insert(0, 'bank', 'RBC')\n",
    "reviewssb = pd.read_json('reviews/com.scotiabank.banking.json')\n",
    "reviewssb.insert(0, 'bank', 'SCOTIA BANK')\n",
    "reviewstd = pd.read_json('reviews/com.td.json')\n",
    "reviewstd.insert(0, 'bank', 'TD')\n",
    "\n",
    "all_reviews = pd.concat([reviewsbmo,reviewscibc,reviewsrbc,reviewssb,reviewstd], ignore_index = True)\n",
    "all_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee15c9de",
   "metadata": {},
   "source": [
    "### Removing unneccesary columns and dropping rows with missing values and saving the cleaned dataframe to a consolidated CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbe74ce2-230b-46cc-ba0a-df3001274f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewsclean = all_reviews.drop(columns = ['reviewId','userName', 'userImage', 'appVersion'])\n",
    "reviewsclean = reviewsclean.dropna(subset = ['reviewCreatedVersion'])\n",
    "reviewsclean.to_csv('all_reviews.csv')\n",
    "reviewsclean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a03a1d",
   "metadata": {},
   "source": [
    "### Preprocessing and NLP operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b69f7979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NLP components\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Add banking-specific stop words\n",
    "banking_stop_words = {\n",
    "    'app', 'banking', 'mobile', 'bank', 'account', 'transaction',\n",
    "    'transfer', 'payment', 'login', 'password', 'username',\n",
    "    'update', 'version', 'download', 'install', 'update'\n",
    "}\n",
    "stop_words.update(banking_stop_words)\n",
    "\n",
    "def load_data(file_path):\n",
    "    print(\"Loading the reviews data...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Loaded {len(df)} reviews\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    return df\n",
    "\n",
    "def basic_cleaning(df):\n",
    "    print(\"Performing basic data cleaning...\")\n",
    "    \n",
    "    # Remove unnamed index column\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df = df.drop('Unnamed: 0', axis=1)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    # Remove rows with missing content\n",
    "    df = df.dropna(subset=['content'])\n",
    "    \n",
    "    # Convert score to int if needed\n",
    "    df['score'] = df['score'].astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def text_cleaning(text):\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "    \n",
    "    # Convert to string\n",
    "    text = str(text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # Remove emojis\n",
    "    text = emoji.replace_emoji(text, '')\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove phone numbers\n",
    "    text = re.sub(r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b', '', text)\n",
    "    \n",
    "    # Remove special characters but keep apostrophes for contractions\n",
    "    text = re.sub(r'[^\\w\\s\\']', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def normalize_text(text):\n",
    "    if not text:\n",
    "        return ''\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove single characters (except 'a' and 'i')\n",
    "    text = re.sub(r'\\b[b-hj-zB-HJ-Z]\\b', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    if not text:\n",
    "        return ''\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"Simplified lemmatization without POS tagging\"\"\"\n",
    "    if not text:\n",
    "        return ''\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "def stem_text(text):\n",
    "    if not text:\n",
    "        return ''\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "def extract_features(df):\n",
    "    print(\"Extracting additional features...\")\n",
    "    \n",
    "    # Text length features\n",
    "    df['content_length'] = df['content'].str.len()\n",
    "    df['content_word_count'] = df['content'].str.split().str.len()\n",
    "    \n",
    "    # Sentiment features using TextBlob\n",
    "    df['sentiment_polarity'] = df['content'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\n",
    "    df['sentiment_subjectivity'] = df['content'].apply(lambda x: TextBlob(str(x)).sentiment.subjectivity)\n",
    "    \n",
    "    # Review type classification\n",
    "    df['review_type'] = df['content_length'].apply(lambda x: 'short' if x < 50 else 'medium' if x < 200 else 'long')\n",
    "    \n",
    "    # Has reply\n",
    "    df['has_reply'] = df['replyContent'].notna().astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def apply_nlp_processing(df):\n",
    "    print(\"Applying NLP processing operations...\")\n",
    "    \n",
    "    # Clean text\n",
    "    df['content_cleaned'] = df['content'].apply(text_cleaning)\n",
    "    \n",
    "    # Normalized text\n",
    "    df['content_cleaned'] = df['content_cleaned'].apply(normalize_text)\n",
    "    \n",
    "    # Text without stopwords\n",
    "    df['content_cleaned'] = df['content_cleaned'].apply(remove_stopwords)\n",
    "    \n",
    "    # Lemmatized text\n",
    "    df['content_cleaned'] = df['content_cleaned'].apply(lemmatize_text)\n",
    "    \n",
    "    # Stemmed text\n",
    "    df['content_cleaned'] = df['content_cleaned'].apply(stem_text)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def analyze_text_quality(df):\n",
    "    print(\"Analyzing text quality metrics...\")\n",
    "    \n",
    "    # Empty reviews\n",
    "    empty_reviews = (df['content_cleaned'] == '').sum()\n",
    "    \n",
    "    # Very short reviews (less than 3 words)\n",
    "    short_reviews = (df['content_word_count'] < 3).sum()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def filter_quality_reviews(df, min_words):\n",
    "    print(f\"Filtering Reviews (min_words={min_words})...\")\n",
    "    \n",
    "    initial_count = len(df)\n",
    "    \n",
    "    # Remove empty or very short reviews\n",
    "    df = df[df['content_word_count'] >= min_words]\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c6b421",
   "metadata": {},
   "source": [
    "### Gathering descriptive statistics about app reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1425a091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_statistics(df):\n",
    "    print(\"Getting comprehensive text statistics...\")\n",
    "    stats = {\n",
    "        'total_reviews': len(df),\n",
    "        'avg_length': df['content_length'].mean(),\n",
    "        'avg_word_count': df['content_word_count'].mean(),\n",
    "        'avg_sentiment': df['sentiment_polarity'].mean(),\n",
    "        'positive_reviews': (df['sentiment_polarity'] > 0.1).sum(),\n",
    "        'negative_reviews': (df['sentiment_polarity'] < -0.1).sum(),\n",
    "        'neutral_reviews': ((df['sentiment_polarity'] >= -0.1) & (df['sentiment_polarity'] <= 0.1)).sum(),\n",
    "        'highly_subjective': (df['sentiment_subjectivity'] > 0.75).sum(),\n",
    "        'mostly_objective': (df['sentiment_subjectivity'] < 0.25).sum(),\n",
    "        'reviews_with_replies': df['has_reply'].sum()\n",
    "    }\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f476b56",
   "metadata": {},
   "source": [
    "### Apply Preprocessing to App Reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0767bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting NLP Preprocessing Pipeline...\n",
      "Loading the reviews data...\n",
      "Loaded 19034 reviews\n",
      "Columns: ['Unnamed: 0', 'bank', 'content', 'score', 'thumbsUpCount', 'reviewCreatedVersion', 'at', 'replyContent', 'repliedAt']\n",
      "Performing basic data cleaning...\n",
      "Extracting additional features...\n",
      "Applying NLP processing operations...\n",
      "Analyzing text quality metrics...\n",
      "Filtering Reviews (min_words=3)...\n",
      "\n",
      "Preprocessing Complete...\n",
      "Final dataset shape: (16068, 15)\n",
      "Processed data saved to 'processed_reviews.csv'\n",
      "\n",
      "Sample of processed data:\n",
      "                                             content  \\\n",
      "0                                        not any use   \n",
      "1  I tried for 3days to access my account but bmo...   \n",
      "2  easy to install. the layout is awesome. no tro...   \n",
      "3                       convenient fast and reliable   \n",
      "4  easy to use and I can do it anywhere. no waiti...   \n",
      "\n",
      "                       content_cleaned  sentiment_polarity  score  \n",
      "0                                  use            0.000000      1  \n",
      "1          tri 3day access bmo respond            0.000000      1  \n",
      "2  easi layout awesom troubl find look            0.511111      5  \n",
      "3                 conveni fast reliabl            0.200000      5  \n",
      "4           easi use anywher wait line            0.433333      4  \n",
      "Getting comprehensive text statistics...\n",
      "\n",
      "Text Statistics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_reviews': 16068,\n",
       " 'avg_length': np.float64(127.10959671396564),\n",
       " 'avg_word_count': np.float64(23.735997012696043),\n",
       " 'avg_sentiment': np.float64(0.11814318405470177),\n",
       " 'positive_reviews': np.int64(7043),\n",
       " 'negative_reviews': np.int64(3254),\n",
       " 'neutral_reviews': np.int64(5771),\n",
       " 'highly_subjective': np.int64(2639),\n",
       " 'mostly_objective': np.int64(3513),\n",
       " 'reviews_with_replies': np.int64(6499)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the complete preprocessing pipeline\n",
    "print(\"Starting NLP Preprocessing Pipeline...\")\n",
    "\n",
    "# Load data\n",
    "df = load_data('all_reviews.csv')\n",
    "\n",
    "# Basic cleaning\n",
    "df = basic_cleaning(df)\n",
    "\n",
    "# Extract features\n",
    "df = extract_features(df)\n",
    "\n",
    "# Apply NLP processing\n",
    "df = apply_nlp_processing(df)\n",
    "\n",
    "# Analyze text quality\n",
    "df = analyze_text_quality(df)\n",
    "\n",
    "# Filter quality reviews\n",
    "df = filter_quality_reviews(df, 3)\n",
    "\n",
    "# Reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(\"\\nPreprocessing Complete...\")\n",
    "print(f\"Final dataset shape: {df.shape}\")\n",
    "\n",
    "# Save processed data\n",
    "df.to_csv('processed_reviews.csv', index=False)\n",
    "print(\"Processed data saved to 'processed_reviews.csv'\")\n",
    "\n",
    "# Display sample of processed data\n",
    "print(\"\\nSample of processed data:\")\n",
    "print(df[['content', 'content_cleaned', 'sentiment_polarity', 'score']].head())\n",
    "\n",
    "# Display text statistics\n",
    "stats = get_text_statistics(df)\n",
    "\n",
    "print(\"\\nText Statistics:\")\n",
    "stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
